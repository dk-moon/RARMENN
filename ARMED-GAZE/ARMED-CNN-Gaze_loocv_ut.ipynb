{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "np.random.seed(530)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow.python.client import device_lib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "# GPU 메모리 증가 허용 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import tf2onnx\n",
    "\n",
    "tf.random.set_seed(530)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpii_path = \"../data/UT/loocv/Fold_3\"\n",
    "\n",
    "# 데이터 로드 및 차원 축소\n",
    "loocv_id_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_ids.npy\")).flatten()\n",
    "loocv_hps_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_2d_hps.npy\")).reshape(-1, 2)\n",
    "loocv_img_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_images.npy\")).reshape(-1, 36, 60)\n",
    "loocv_gzs_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_2d_gazes.npy\")).reshape(-1, 2)\n",
    "\n",
    "loocv_id_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_ids.npy\")).flatten()\n",
    "loocv_hps_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_2d_hps.npy\")).reshape(-1, 2)\n",
    "loocv_img_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_images.npy\")).reshape(-1, 36, 60)\n",
    "loocv_gzs_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_2d_gazes.npy\")).reshape(-1, 2)\n",
    "\n",
    "# 전체 ID 데이터 집합 생성\n",
    "total_id_data = np.unique(np.concatenate([loocv_id_data_tr, loocv_id_data_te]))\n",
    "\n",
    "# Label Encoder와 OneHot Encoder 초기화\n",
    "lb_encoder = LabelEncoder()\n",
    "oh_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# 전체 ID에 대해 Label Encoding\n",
    "lb_encoder.fit(total_id_data)  # Label encoding을 위한 전체 ID 학습\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "oh_encoder.fit(lb_encoder.transform(total_id_data).reshape(-1, 1))\n",
    "\n",
    "# 학습 데이터와 추론 데이터에 대한 ID 원-핫 인코딩\n",
    "train_id_encoded = oh_encoder.transform(lb_encoder.transform(loocv_id_data_tr).reshape(-1, 1))\n",
    "test_id_encoded = oh_encoder.transform(lb_encoder.transform(loocv_id_data_te).reshape(-1, 1))\n",
    "\n",
    "# 이미지 데이터 정규화\n",
    "train_img_normalized = loocv_img_data_tr / 255.0\n",
    "test_img_normalized = loocv_img_data_te / 255.0\n",
    "\n",
    "# 데이터셋 구성\n",
    "train_data = ((train_img_normalized, loocv_hps_data_tr, train_id_encoded), loocv_gzs_data_tr)\n",
    "test_data = ((test_img_normalized, loocv_hps_data_te, test_id_encoded), loocv_gzs_data_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRegressor(keras.Model):\n",
    "    '''\n",
    "    Simple 2D image regressor with 7 convolution blocks and 2 final dense layers.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, name='regressor', **kwargs):\n",
    "        \"\"\"Simple 2D image regressor with 7 convolution blocks and 2 final dense layers.\n",
    "\n",
    "        Args:\n",
    "            name (str, optional): Model name. Defaults to 'regressor'.\n",
    "        \"\"\"        \n",
    "        \n",
    "        super(ImageRegressor, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.elu0 = keras.layers.ELU(name='elu0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(128, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.elu1 = keras.layers.ELU(name='elu1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.elu2 = keras.layers.ELU(name='elu2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.elu3 = keras.layers.ELU(name='elu3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.elu4 = keras.layers.ELU(name='elu4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(512, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.elu5 = keras.layers.ELU(name='elu5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.elu6 = keras.layers.ELU(name='elu6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu',\n",
    "                                        kernel_regularizer=keras.regularizers.L1L2(l1=0.01))\n",
    "        self.out = keras.layers.Dense(2, name='output')\n",
    "        \n",
    "    def call(self, inputs, return_layer_activations=False):\n",
    "        images, head_poses = inputs\n",
    "        \n",
    "        c0 = self.conv0(images)\n",
    "        c0 = self.bn0(c0)\n",
    "        c0 = self.elu0(c0)\n",
    "        \n",
    "        c1 = self.maxpool0(c0)\n",
    "        c1 = self.conv1(c1)\n",
    "        c1 = self.dropout1(c1)\n",
    "        c1 = self.bn1(c1)\n",
    "        c1 = self.elu1(c1)\n",
    "        \n",
    "        c2 = self.maxpool1(c1)\n",
    "        c2 = self.conv2(c2)\n",
    "        c2 = self.bn2(c2)\n",
    "        c2 = self.elu2(c2)\n",
    "        \n",
    "        c3 = self.maxpool2(c2)\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = self.dropout3(c3)\n",
    "        c3 = self.bn3(c3)\n",
    "        c3 = self.elu3(c3)\n",
    "        \n",
    "        c4 = self.maxpool3(c3)\n",
    "        c4 = self.conv4(c4)\n",
    "        c4 = self.bn4(c4)\n",
    "        c4 = self.elu4(c4)\n",
    "        \n",
    "        c5 = self.maxpool4(c4)\n",
    "        c5 = self.conv5(c5)\n",
    "        c5 = self.dropout5(c5)\n",
    "        c5 = self.bn5(c5)\n",
    "        c5 = self.elu5(c5)\n",
    "        \n",
    "        c6 = self.maxpool5(c5)\n",
    "        c6 = self.conv6(c6)\n",
    "        c6 = self.elu6(c6)\n",
    "        h = self.flatten(c6)\n",
    "        h = keras.layers.Concatenate()([h, head_poses])\n",
    "        h = self.dense(h)\n",
    "        y = self.out(h)\n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialRegressor(keras.Model):\n",
    "    '''\n",
    "    Domain adversarial regressor for the ImageRegressor. Receives the\n",
    "    layer activations from the ImageRegressor as inputs and predicts\n",
    "    cluster membership.\n",
    "    '''\n",
    "    def __init__(self, n_clusters, name='adversary', **kwargs):\n",
    "        \"\"\"Domain adversarial regressor for the ImageRegressor. Receives the\n",
    "        layer activations from the ImageRegressor as inputs and predicts\n",
    "        cluster membership.\n",
    "\n",
    "        Args: \n",
    "            n_clusters (int): number of clusters \n",
    "            name (str, optional): Model name. Defaults to 'adversary'.\n",
    "        \"\"\"        \n",
    "        super(AdversarialRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.elu0 = keras.layers.ELU(name='elu0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.elu1 = keras.layers.ELU(name='elu1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.elu2 = keras.layers.ELU(name='elu2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(128, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.elu3 = keras.layers.ELU(name='elu3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.elu4 = keras.layers.ELU(name='elu4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(256, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.elu5 = keras.layers.ELU(name='elu5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.elu6 = keras.layers.ELU(name='elu6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu')\n",
    "        self.out = keras.layers.Dense(n_clusters, name='output', activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        c0, c1, c2, c3, c4, c5, c6, h = inputs\n",
    "        \n",
    "        x = self.conv0(c0)\n",
    "        x = self.bn0(x)\n",
    "        x = self.elu0(x)\n",
    "        \n",
    "        x = self.maxpool0(x)\n",
    "        x = keras.layers.Concatenate()([x, c1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu1(x)\n",
    "        \n",
    "        x = self.maxpool1(x)\n",
    "        x = keras.layers.Concatenate()([x, c2])\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu2(x)\n",
    "        \n",
    "        x = self.maxpool2(x)\n",
    "        x = keras.layers.Concatenate()([x, c3])\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu3(x)\n",
    "        \n",
    "        x = self.maxpool3(x)\n",
    "        x = keras.layers.Concatenate()([x, c4])\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.elu4(x)\n",
    "        \n",
    "        x = self.maxpool4(x)\n",
    "        x = keras.layers.Concatenate()([x, c5])\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.elu5(x)\n",
    "        \n",
    "        x = self.maxpool5(x)\n",
    "        # Don't concatenate c6 because the tensor shapes don't line up\n",
    "        x = self.conv6(x)\n",
    "        x = self.elu6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = keras.layers.Concatenate()([x, h])\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effects Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior_fn(prior_scale):\n",
    "    def _prior_fn(kernel_size, bias_size=0, prior_scale=prior_scale):\n",
    "        n = kernel_size + bias_size\n",
    "        prior_loc = tf.zeros(n, dtype=tf.float32)\n",
    "        prior_scale = tf.fill([n], tf.cast(prior_scale, tf.float32))\n",
    "        \n",
    "        def prior_fn():\n",
    "            return tf.concat([prior_loc, prior_scale], axis=-1)\n",
    "        \n",
    "        return prior_fn\n",
    "    return _prior_fn\n",
    "\n",
    "def make_posterior_fn(post_loc_init_scale, post_scale_init_min, post_scale_init_range):\n",
    "    def _re_posterior_fn(kernel_size, bias_size=0):\n",
    "        n = kernel_size + bias_size\n",
    "        initializer = keras.initializers.RandomNormal(mean=0, stddev=post_loc_init_scale)\n",
    "        posterior_loc = tf.Variable(initial_value=initializer(shape=(n,), dtype=tf.float32), trainable=True)\n",
    "        initializer = keras.initializers.RandomUniform(minval=post_scale_init_min, \n",
    "                                                          maxval=post_scale_init_min + post_scale_init_range)\n",
    "        posterior_scale = tf.Variable(initial_value=initializer(shape=(n,), dtype=tf.float32), trainable=True)\n",
    "        \n",
    "        def posterior_fn():\n",
    "            return tf.concat([posterior_loc, tf.nn.softplus(posterior_scale)], axis=-1)\n",
    "        \n",
    "        return posterior_fn\n",
    "    return _re_posterior_fn\n",
    "\n",
    "class RandomEffects(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 units: int=1, \n",
    "                 post_loc_init_scale: float=0.05, \n",
    "                 post_scale_init_min: float=0.05,\n",
    "                 post_scale_init_range: float=0.05,\n",
    "                 prior_scale: float=0.05,\n",
    "                 kl_weight: float=0.001,\n",
    "                 l1_weight: float=None,\n",
    "                 name=None) -> None:  \n",
    "        \n",
    "        super(RandomEffects, self).__init__(name=name)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.units = units\n",
    "        self.prior_scale = prior_scale\n",
    "        \n",
    "        posterior_fn = make_posterior_fn(post_loc_init_scale, post_scale_init_min, post_scale_init_range)\n",
    "        self.posterior = posterior_fn(units)\n",
    "        \n",
    "        prior_fn = make_prior_fn(prior_scale)\n",
    "        self.prior = prior_fn(units, prior_scale=prior_scale)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)  # 데이터 타입을 float32로 변경\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)  # inputs의 차원을 확장\n",
    "\n",
    "        if training:\n",
    "            # sample from approximate posterior\n",
    "            posterior_sample = self.posterior()\n",
    "            loc = posterior_sample[..., :self.units]\n",
    "            scale = tf.nn.softplus(posterior_sample[..., self.units:])  # Apply softplus to ensure positive scale\n",
    "\n",
    "            eps = tf.random.normal(shape=(tf.shape(inputs)[0], self.units), mean=0, stddev=1)\n",
    "            u = loc + eps * scale\n",
    "            u = tf.expand_dims(u, axis=1)  # u의 형태를 [32, 1, 512]로 조정\n",
    "            outputs = tf.matmul(inputs, u)  # 변경된 차원으로 곱셈 수행\n",
    "            # outputs = tf.squeeze(outputs, axis=1)  # 결과 텐서에서 불필요한 차원 제거\n",
    "\n",
    "            # compute kl divergence\n",
    "            prior_sample = self.prior()\n",
    "            prior_loc = prior_sample[..., :self.units]\n",
    "            prior_scale = tf.nn.softplus(prior_sample[..., self.units:])\n",
    "            \n",
    "            kl = 0.5 * tf.reduce_sum(\n",
    "                tf.square(loc - prior_loc) / tf.square(prior_scale) +\n",
    "                tf.square(scale) / tf.square(prior_scale) -\n",
    "                1.0 + 2.0 * (tf.math.log(prior_scale) - tf.math.log(scale))\n",
    "            )\n",
    "            kl = kl * self.kl_weight\n",
    "            self.add_loss(kl)\n",
    "        else:\n",
    "            # In testing mode, use the posterior means\n",
    "            posterior_sample = self.posterior()\n",
    "            loc = posterior_sample[..., :self.units]\n",
    "            loc = tf.expand_dims(loc, axis=0)  # Ensure loc has the correct shape for matmul\n",
    "            loc = tf.expand_dims(loc, axis=1)  # loc 형태를 [1, 512, 1]로 조정\n",
    "            outputs = tf.matmul(inputs, loc)\n",
    "            # outputs = tf.squeeze(outputs, axis=2)  # 결과 텐서에서 불필요한 차원 제거\n",
    "\n",
    "        if self.l1_weight:\n",
    "            self.add_loss(self.l1_weight * tf.reduce_sum(tf.abs(loc)))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adversarial Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdversarialImageRegressor(keras.Model):\n",
    "    '''\n",
    "    Domain adversarial 2D image regressor which learns the regression \n",
    "    task while competing with an adversary, which learns to predict cluster \n",
    "    membership from the regressor's layer activations.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_clusters, name='da_regressor', **kwargs):\n",
    "        \"\"\"Domain adversarial 2D image regressor which learns the regression \n",
    "        task while competing with an adversary, which learns to predict cluster \n",
    "        membership from the regressor's layer activations.\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): number of clusters\n",
    "            name (str, optional): Model name. Defaults to 'da_regressor'.\n",
    "        \"\"\"        \n",
    "        super(DomainAdversarialImageRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.regressor = ImageRegressor()\n",
    "        self.adversary = AdversarialRegressor(n_clusters)\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, hps, z, = inputs\n",
    "        y_pred = self.regressor((x,hps))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def compile(self,\n",
    "                loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "                loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "                loss_regressor_weight=1.0,\n",
    "                loss_gen_weight=0.1,\n",
    "                metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "                opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "                opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "                ):\n",
    "      \n",
    "        super().compile()\n",
    "        \n",
    "        self.loss_regressor = loss_regressor\n",
    "        self.loss_adversary = loss_adversary\n",
    "        self.loss_regressor_weight = loss_regressor_weight\n",
    "        self.loss_gen_weight = loss_gen_weight\n",
    "        self.metric_regressor = metric_regressor\n",
    "        self.opt_regressor = opt_regressor\n",
    "        self.opt_adversary = opt_adversary\n",
    "        \n",
    "        self.loss_regressor_tracker = keras.metrics.Mean(name='reg_loss')\n",
    "        self.loss_gen_tracker = keras.metrics.Mean(name='gen_loss')\n",
    "        self.loss_adversary_tracker = keras.metrics.Mean(name='adv_loss')\n",
    "        self.loss_total_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "        \n",
    "    def _compute_update_loss(self, loss_reg, loss_gen):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "        \n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) + (self.loss_gen_weight * loss_gen)\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            (images, headpose, clusters), labels, sample_weights = data\n",
    "        else:\n",
    "            (images, headpose, clusters), labels = data\n",
    "            sample_weights = None\n",
    "            \n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "        \n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            loss_reg = self.loss_regressor(labels, y_pred, sample_weight=sample_weights)\n",
    "                        \n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "            \n",
    "            loss_total = self._compute_update_loss(loss_reg, loss_gen)\n",
    "            \n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        \n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "        \n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "        \n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "        \n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "        \n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen)\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Effects Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEffectsRegressor(DomainAdversarialImageRegressor):\n",
    "    def __init__(self, \n",
    "                 cluster_list=[],\n",
    "                 slope_post_init_scale=0.1,\n",
    "                 intercept_post_init_scale=0.1,\n",
    "                 slope_prior_scale=0.25,\n",
    "                 intercept_prior_scale=0.25,\n",
    "                 kl_weight=1e-3,\n",
    "                 name='me_regressor', **kwargs):\n",
    "\n",
    "        # self.cluster_list = cluster_list\n",
    "        self.cluster_list = tf.constant([int(cid[1:]) for cid in cluster_list], dtype=tf.int64)\n",
    "        n_clusters = len(self.cluster_list)\n",
    "        super(MixedEffectsRegressor, self).__init__(n_clusters, name=name, **kwargs)\n",
    "                \n",
    "        # Single slope and intercept RE layer\n",
    "        self.re_slopes = RandomEffects(units=256,\n",
    "                                       post_loc_init_scale=slope_post_init_scale,\n",
    "                                       prior_scale=slope_prior_scale,\n",
    "                                       kl_weight=kl_weight,\n",
    "                                       name='re_slopes')\n",
    "        \n",
    "        self.re_intercept = RandomEffects(units=2,\n",
    "                                       post_loc_init_scale=intercept_post_init_scale,\n",
    "                                       prior_scale=intercept_prior_scale,\n",
    "                                       kl_weight=kl_weight,\n",
    "                                       name='re_intercept')\n",
    "        \n",
    "    def call(self, inputs, training, return_layer_activations=False):\n",
    "        x, hps, z = inputs\n",
    "        \n",
    "        if x.shape[-1] != 1:\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        c0 = self.regressor.conv0(x)\n",
    "        c0 = self.regressor.bn0(c0)\n",
    "        c0 = self.regressor.elu0(c0)\n",
    "        \n",
    "        c1 = self.regressor.maxpool0(c0)\n",
    "        c1 = self.regressor.conv1(c1)\n",
    "        c1 = self.regressor.dropout1(c1)\n",
    "        c1 = self.regressor.bn1(c1)\n",
    "        c1 = self.regressor.elu1(c1)\n",
    "        \n",
    "        c2 = self.regressor.maxpool1(c1)\n",
    "        c2 = self.regressor.conv2(c2)\n",
    "        c2 = self.regressor.bn2(c2)\n",
    "        c2 = self.regressor.elu2(c2)\n",
    "        \n",
    "        c3 = self.regressor.maxpool2(c2)\n",
    "        c3 = self.regressor.conv3(c3)\n",
    "        c3 = self.regressor.dropout3(c3)\n",
    "        c3 = self.regressor.bn3(c3)\n",
    "        c3 = self.regressor.elu3(c3)\n",
    "        \n",
    "        c4 = self.regressor.maxpool3(c3)\n",
    "        c4 = self.regressor.conv4(c4)\n",
    "        c4 = self.regressor.bn4(c4)\n",
    "        c4 = self.regressor.elu4(c4)\n",
    "        \n",
    "        c5 = self.regressor.maxpool4(c4)\n",
    "        c5 = self.regressor.conv5(c5)\n",
    "        c5 = self.regressor.dropout5(c5)\n",
    "        c5 = self.regressor.bn5(c5)\n",
    "        c5 = self.regressor.elu5(c5)\n",
    "        \n",
    "        c6 = self.regressor.maxpool5(c5)\n",
    "        c6 = self.regressor.conv6(c6)\n",
    "        c6 = self.regressor.elu6(c6)\n",
    "        h = self.regressor.flatten(c6)\n",
    "        h = tf.concat([h, hps], axis=-1)\n",
    "        h = self.regressor.dense(h)\n",
    "        \n",
    "        slopes = self.re_slopes(z, training=training)\n",
    "        intercepts = self.re_intercept(z, training=training)\n",
    "        if len(slopes.shape) == 3:\n",
    "            slopes = tf.reduce_mean(slopes, axis=1)\n",
    "        if len(intercepts.shape) == 3:\n",
    "            intercepts = tf.reduce_mean(intercepts, axis=1)\n",
    "        y = self.regressor.out(h * (1 + slopes))\n",
    "\n",
    "        # Apply intercepts\n",
    "        y = y + intercepts\n",
    "        \n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def compile(self,\n",
    "            loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "            loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "            loss_regressor_weight=1.0,\n",
    "            loss_gen_weight=0.1,\n",
    "            metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "            opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "            opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "            ):\n",
    "        \n",
    "        super(MixedEffectsRegressor, self).compile(loss_regressor=loss_regressor,\n",
    "                                                    loss_adversary=loss_adversary,\n",
    "                                                    loss_regressor_weight=loss_regressor_weight,\n",
    "                                                    loss_gen_weight=loss_gen_weight,\n",
    "                                                    metric_regressor=metric_regressor,\n",
    "                                                    opt_regressor=opt_regressor,\n",
    "                                                    opt_adversary=opt_adversary)\n",
    "        self.loss_kld_tracker = tf.keras.metrics.Mean(name='kld')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_kld_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "        \n",
    "    def _compute_update_loss(self, loss_reg, loss_gen, training=True):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "        if training:\n",
    "            kld = tf.reduce_sum(self.re_slopes.losses) + tf.reduce_sum(self.re_intercept.losses)\n",
    "            self.loss_kld_tracker.update_state(kld)\n",
    "        else:\n",
    "            # KLD can't be computed at inference time because posteriors are simplified to \n",
    "            # point estimates\n",
    "            kld = 0\n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) \\\n",
    "            + (self.loss_gen_weight * loss_gen) \\\n",
    "            + kld\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "        \n",
    "        return loss_total\n",
    "        \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        sample_weights = None\n",
    "\n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "\n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            # 각 출력에 대한 손실을 계산하고 합산\n",
    "            total_loss_reg = 0\n",
    "            for i in range(y_pred.shape[1]): # y_pred의 출력 개수만큼 반복\n",
    "                total_loss_reg += self.loss_regressor(labels[:, i], y_pred[:, i], sample_weight=sample_weights)\n",
    "\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "            loss_total = self._compute_update_loss(total_loss_reg, loss_gen)\n",
    "\n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        \n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "        \n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "        \n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "        \n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "        \n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen)\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = MixedEffectsRegressor(cluster_list = total_id_data)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "              loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "              loss_regressor_weight=1.0,\n",
    "              loss_gen_weight=0.1,\n",
    "              metric_regressor=tf.keras.metrics.MeanAbsoluteError(),\n",
    "              opt_regressor=tf.keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "              opt_adversary=tf.keras.optimizers.Nadam(learning_rate=0.0001))\n",
    "\n",
    "# 콜백 설정\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    x=train_data[0],  # 입력 데이터: 이미지와 클러스터 정보\n",
    "    y=train_data[1],  # 타겟 데이터: 시선 벡터\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    #validation_data=val_data,  # 검증 데이터\n",
    "    # callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = ((test_img_normalized, loocv_hps_data_te, test_id_encoded), loocv_gzs_data_te)\n",
    "pred_gzs = model.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_keras(predictedGaze, groundtruthGaze, is_3d=False, deg=False):\n",
    "    '''\n",
    "    Calculate Mean Angular Error using TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "    predictedGaze (tf.Tensor): Predicted gaze vectors.\n",
    "    groundtruthGaze (tf.Tensor): Ground truth gaze vectors.\n",
    "    is_3d (bool): Flag indicating whether the input vectors are 3D. Default is False.\n",
    "    deg (bool): Flag indicating whether the spherical coordinates are in degrees. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    tf.Tensor: Mean angular error.\n",
    "    '''\n",
    "    Gaze_1 = tf.cast(predictedGaze, dtype=tf.float32)\n",
    "    Gaze_2 = tf.cast(groundtruthGaze, dtype=tf.float32)\n",
    "    \n",
    "    if not is_3d:\n",
    "        Gaze_1 = convert_to_xyz_tf(Gaze_1, deg)\n",
    "        Gaze_2 = convert_to_xyz_tf(Gaze_2, deg)\n",
    "\n",
    "    Gaze_1 = Gaze_1 / tf.norm(Gaze_1, axis=1, keepdims=True)\n",
    "    Gaze_2 = Gaze_2 / tf.norm(Gaze_2, axis=1, keepdims=True)\n",
    "\n",
    "    cos_val = tf.reduce_sum(Gaze_1 * Gaze_2, axis=1)\n",
    "    cos_val = tf.clip_by_value(cos_val, -1, 1)  # Ensure cos_val is within the valid range for arccos\n",
    "\n",
    "    angle_val = tf.acos(cos_val) * 180 / tf.constant(np.pi)\n",
    "\n",
    "    return tf.reduce_mean(angle_val)\n",
    "\n",
    "def convert_to_xyz_tf(spherical, deg=False):\n",
    "    if deg:\n",
    "        spherical = spherical * tf.constant(np.pi) / 180\n",
    "\n",
    "    # Create xyz tensor similarly as above but using TensorFlow operations.\n",
    "    xyz = tf.zeros((spherical.shape[0], 3), dtype=tf.float32)\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 0] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.sin(spherical[:, 1]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 1] for i in range(spherical.shape[0])],\n",
    "        -tf.sin(spherical[:, 0]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 2] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.cos(spherical[:, 1]))\n",
    "\n",
    "    xyz = xyz / tf.norm(xyz, axis=1, keepdims=True)\n",
    "\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE 계산\n",
    "error = mae_keras(pred_gzs, test_data[1])\n",
    "print(\"Mean Angular Error:\", error.numpy(), \"degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
