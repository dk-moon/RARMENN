{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "np.random.seed(530)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow.python.client import device_lib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "# GPU 메모리 증가 허용 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import tf2onnx\n",
    "\n",
    "tf.random.set_seed(530)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpii_path = \"../data/UT/loocv/Fold_3\"\n",
    "\n",
    "# 데이터 로드 및 차원 축소\n",
    "loocv_id_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_ids.npy\")).flatten()\n",
    "loocv_hps_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_2d_hps.npy\")).reshape(-1, 2)\n",
    "loocv_img_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_images.npy\")).reshape(-1, 36, 60)\n",
    "loocv_gzs_data_tr = np.load(os.path.join(mpii_path, \"utm_fold_3_train_2d_gazes.npy\")).reshape(-1, 2)\n",
    "\n",
    "loocv_id_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_ids.npy\")).flatten()\n",
    "loocv_hps_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_2d_hps.npy\")).reshape(-1, 2)\n",
    "loocv_img_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_images.npy\")).reshape(-1, 36, 60)\n",
    "loocv_gzs_data_te = np.load(os.path.join(mpii_path, \"utm_fold_3_test_2d_gazes.npy\")).reshape(-1, 2)\n",
    "\n",
    "# 전체 ID 데이터 집합 생성\n",
    "total_id_data = np.unique(np.concatenate([loocv_id_data_tr, loocv_id_data_te]))\n",
    "\n",
    "# Label Encoder와 OneHot Encoder 초기화\n",
    "lb_encoder = LabelEncoder()\n",
    "oh_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# 전체 ID에 대해 Label Encoding\n",
    "lb_encoder.fit(total_id_data)  # Label encoding을 위한 전체 ID 학습\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "oh_encoder.fit(lb_encoder.transform(total_id_data).reshape(-1, 1))\n",
    "\n",
    "# 학습 데이터와 추론 데이터에 대한 ID 원-핫 인코딩\n",
    "train_id_encoded = oh_encoder.transform(lb_encoder.transform(loocv_id_data_tr).reshape(-1, 1))\n",
    "test_id_encoded = oh_encoder.transform(lb_encoder.transform(loocv_id_data_te).reshape(-1, 1))\n",
    "\n",
    "# 이미지 데이터 정규화\n",
    "train_img_normalized = loocv_img_data_tr / 255.0\n",
    "test_img_normalized = loocv_img_data_te / 255.0\n",
    "\n",
    "# 데이터셋 구성\n",
    "train_data = ((train_img_normalized, loocv_hps_data_tr, train_id_encoded), loocv_gzs_data_tr)\n",
    "test_data = ((test_img_normalized, loocv_hps_data_te, test_id_encoded), loocv_gzs_data_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRegressor(keras.Model):\n",
    "    def __init__(self, name='regressor', **kwargs):\n",
    "        super(ImageRegressor, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.elu0 = keras.layers.ELU(name='elu0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30\n",
    "        self.conv1 = keras.layers.Conv2D(128, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.elu1 = keras.layers.ELU(name='elu1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.elu2 = keras.layers.ELU(name='elu2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.elu3 = keras.layers.ELU(name='elu3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.elu4 = keras.layers.ELU(name='elu4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(512, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.elu5 = keras.layers.ELU(name='elu5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.elu6 = keras.layers.ELU(name='elu6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu',\n",
    "                                        kernel_regularizer=keras.regularizers.L1L2(l1=0.01))\n",
    "        self.out = keras.layers.Dense(2, name='output')\n",
    "\n",
    "    def call(self, inputs, return_layer_activations=False):\n",
    "        images, head_poses = inputs\n",
    "\n",
    "        c0 = self.conv0(images)\n",
    "        c0 = self.bn0(c0)\n",
    "        c0 = self.elu0(c0)\n",
    "\n",
    "        c1 = self.maxpool0(c0)\n",
    "        c1 = self.conv1(c1)\n",
    "        c1 = self.dropout1(c1)\n",
    "        c1 = self.bn1(c1)\n",
    "        c1 = self.elu1(c1)\n",
    "\n",
    "        c2 = self.maxpool1(c1)\n",
    "        c2 = self.conv2(c2)\n",
    "        c2 = self.bn2(c2)\n",
    "        c2 = self.elu2(c2)\n",
    "\n",
    "        c3 = self.maxpool2(c2)\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = self.dropout3(c3)\n",
    "        c3 = self.bn3(c3)\n",
    "        c3 = self.elu3(c3)\n",
    "\n",
    "        c4 = self.maxpool3(c3)\n",
    "        c4 = self.conv4(c4)\n",
    "        c4 = self.bn4(c4)\n",
    "        c4 = self.elu4(c4)\n",
    "\n",
    "        c5 = self.maxpool4(c4)\n",
    "        c5 = self.conv5(c5)\n",
    "        c5 = self.dropout5(c5)\n",
    "        c5 = self.bn5(c5)\n",
    "        c5 = self.elu5(c5)\n",
    "\n",
    "        c6 = self.maxpool5(c5)\n",
    "        c6 = self.conv6(c6)\n",
    "        c6 = self.elu6(c6)\n",
    "        h = self.flatten(c6)\n",
    "        h = keras.layers.Concatenate()([h, head_poses])\n",
    "        h = self.dense(h)\n",
    "        y = self.out(h)\n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialRegressor(keras.Model):\n",
    "    def __init__(self, n_clusters, name='adversary', **kwargs):\n",
    "        super(AdversarialRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.elu0 = keras.layers.ELU(name='elu0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.elu1 = keras.layers.ELU(name='elu1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.elu2 = keras.layers.ELU(name='elu2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(128, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.elu3 = keras.layers.ELU(name='elu3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.elu4 = keras.layers.ELU(name='elu4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(256, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.elu5 = keras.layers.ELU(name='elu5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.elu6 = keras.layers.ELU(name='elu6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu')\n",
    "        self.out = keras.layers.Dense(n_clusters, name='output', activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Fixed Effect Subnetwork의 Feature map\n",
    "        c0, c1, c2, c3, c4, c5, c6, h = inputs\n",
    "\n",
    "        x = self.conv0(c0)\n",
    "        x = self.bn0(x)\n",
    "        x = self.elu0(x)\n",
    "        x = self.maxpool0(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c2])\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c3])\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c4])\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.elu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c5])\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.elu5(x)\n",
    "\n",
    "        x = self.maxpool5(x)\n",
    "        # Don't concatenate c6 because the tensor shapes don't line up\n",
    "        x = self.conv6(x)\n",
    "        x = self.elu6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = keras.layers.Concatenate()([x, h])\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effects Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior_fn(df, scale):\n",
    "    def prior_fn():\n",
    "        return tfd.StudentT(df=df, loc=0.0, scale=scale)\n",
    "    return prior_fn\n",
    "\n",
    "def make_posterior_fn(df, loc_init_scale, scale_init_min, scale_init_range):\n",
    "    def posterior_fn(units):\n",
    "        loc_initializer = tf.random_normal_initializer(mean=0.0, stddev=loc_init_scale)\n",
    "        scale_initializer = tf.random_uniform_initializer(minval=scale_init_min, maxval=scale_init_min + scale_init_range)\n",
    "\n",
    "        loc = tf.Variable(initial_value=loc_initializer(shape=(units,)), name=\"loc\", dtype=tf.float32)\n",
    "        scale = tf.Variable(initial_value=scale_initializer(shape=(units,)), name=\"scale\", dtype=tf.float32)\n",
    "\n",
    "        return tfd.StudentT(df=df, loc=loc, scale=tf.nn.softplus(scale))\n",
    "    return posterior_fn\n",
    "\n",
    "def kl_divergence_student_t(posterior, prior, num_samples=1000, seed=None):\n",
    "    \"\"\"Calculate the KL divergence between two Student's T distributions using Monte Carlo approximation.\"\"\"\n",
    "    samples = posterior.sample(num_samples, seed=seed)  # Sample from the posterior\n",
    "    log_posterior_prob = posterior.log_prob(samples)  # Log probability under the posterior\n",
    "    log_prior_prob = prior.log_prob(samples)  # Log probability under the prior\n",
    "\n",
    "    # Monte Carlo approximation of the KL divergence\n",
    "    kl_div = tf.reduce_mean(log_posterior_prob - log_prior_prob)\n",
    "    return kl_div\n",
    "\n",
    "class RandomEffects(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 df,\n",
    "                 post_loc_init_scale,\n",
    "                 post_scale_init_min,\n",
    "                 post_scale_init_range,\n",
    "                 prior_scale,\n",
    "                 kl_weight=0.001,\n",
    "                 l1_weight=None,\n",
    "                 name=None):\n",
    "\n",
    "        super(RandomEffects, self).__init__(name=name)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.units = units\n",
    "\n",
    "        self.posterior = make_posterior_fn(df, post_loc_init_scale, post_scale_init_min, post_scale_init_range)(units)\n",
    "        self.prior = make_prior_fn(df, prior_scale)()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)  # 데이터 타입을 float32로 변경\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)  # inputs의 차원을 확장\n",
    "        if training:\n",
    "            # Shape of `samples`: [batch_size, units]\n",
    "            samples = self.posterior.sample(sample_shape=(tf.shape(inputs)[0],))\n",
    "            # Ensure 'samples' can be broadcasted with 'inputs'\n",
    "            # Assuming `inputs` shape is [batch_size, input_dim], we need to align 'samples' along that dimension\n",
    "            samples = tf.reshape(samples, [tf.shape(inputs)[0], 1, self.units])\n",
    "            outputs = inputs * samples  # Broadcast multiplication\n",
    "\n",
    "            kl_div = kl_divergence_student_t(self.posterior, self.prior, num_samples=1000, seed=42)\n",
    "            self.add_loss(self.kl_weight * tf.reduce_sum(kl_div))\n",
    "        else:\n",
    "            # Use the mean of the posterior distribution as a deterministic output\n",
    "            mean_samples = self.posterior.mean()\n",
    "            mean_samples = tf.reshape(mean_samples, [1, 1, self.units])\n",
    "            outputs = inputs * mean_samples  # Broadcast multiplication\n",
    "\n",
    "        if self.l1_weight:\n",
    "            self.add_loss(self.l1_weight * tf.reduce_sum(tf.abs(self.posterior.mean())))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adversarial Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdversarialImageRegressor(keras.Model):\n",
    "\n",
    "    def __init__(self, n_clusters, name='da_regressor', **kwargs):\n",
    "        super(DomainAdversarialImageRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.regressor = ImageRegressor()\n",
    "        self.adversary = AdversarialRegressor(n_clusters)\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, hps, z, = inputs\n",
    "        y_pred = self.regressor((x,hps))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def compile(self,\n",
    "                loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "                loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "                loss_regressor_weight=1.0,\n",
    "                loss_gen_weight=0.1,\n",
    "                metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "                opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "                opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "                ):\n",
    "\n",
    "        super().compile()\n",
    "\n",
    "        self.loss_regressor = loss_regressor\n",
    "        self.loss_adversary = loss_adversary\n",
    "        self.loss_regressor_weight = loss_regressor_weight\n",
    "        self.loss_gen_weight = loss_gen_weight\n",
    "        self.metric_regressor = metric_regressor\n",
    "        self.opt_regressor = opt_regressor\n",
    "        self.opt_adversary = opt_adversary\n",
    "\n",
    "        self.loss_regressor_tracker = keras.metrics.Mean(name='reg_loss')\n",
    "        self.loss_gen_tracker = keras.metrics.Mean(name='gen_loss')\n",
    "        self.loss_adversary_tracker = keras.metrics.Mean(name='adv_loss')\n",
    "        self.loss_total_tracker = keras.metrics.Mean(name='total_loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "\n",
    "    def _compute_update_loss(self, loss_reg, loss_gen):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "\n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) + (self.loss_gen_weight * loss_gen)\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            (images, headpose, clusters), labels, sample_weights = data\n",
    "        else:\n",
    "            (images, headpose, clusters), labels = data\n",
    "            sample_weights = None\n",
    "\n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "\n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            loss_reg = self.loss_regressor(labels, y_pred, sample_weight=sample_weights)\n",
    "\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "            loss_total = self._compute_update_loss(loss_reg, loss_gen)\n",
    "\n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "\n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "\n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "\n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "\n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "\n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen)\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Effects Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEffectsRegressor(DomainAdversarialImageRegressor):\n",
    "    def __init__(self,\n",
    "                 cluster_list=[],\n",
    "                 slope_post_init_scale=0.1,\n",
    "                 intercept_post_init_scale=0.1,\n",
    "                 slope_scale=0.25,\n",
    "                 intercept_scale=0.25,\n",
    "                 kl_weight=1e-3,\n",
    "                 df=10,\n",
    "                 name='me_regressor', **kwargs):\n",
    "\n",
    "        self.cluster_list = tf.constant([int(cid[1:]) for cid in cluster_list], dtype=tf.int64)\n",
    "        n_clusters = len(self.cluster_list)\n",
    "        super(MixedEffectsRegressor, self).__init__(n_clusters, name=name, **kwargs)\n",
    "\n",
    "        self.re_slopes = RandomEffects(units=256,\n",
    "                                       df=df,\n",
    "                                       post_loc_init_scale=slope_post_init_scale,\n",
    "                                       post_scale_init_min=0.01,\n",
    "                                       post_scale_init_range=0.02,\n",
    "                                       prior_scale=slope_scale,\n",
    "                                       kl_weight=kl_weight,\n",
    "                                       name='re_slopes')\n",
    "\n",
    "        self.re_intercept = RandomEffects(units=2,\n",
    "                                          df=df,\n",
    "                                          post_loc_init_scale=intercept_post_init_scale,\n",
    "                                          post_scale_init_min=0.01,\n",
    "                                          post_scale_init_range=0.02,\n",
    "                                          prior_scale=intercept_scale,\n",
    "                                          kl_weight=kl_weight,\n",
    "                                          name='re_intercept')\n",
    "\n",
    "    def call(self, inputs, training, return_layer_activations=False):\n",
    "        x, hps, z = inputs\n",
    "\n",
    "        if x.shape[-1] != 1:\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        c0 = self.regressor.conv0(x)\n",
    "        c0 = self.regressor.bn0(c0)\n",
    "        c0 = self.regressor.elu0(c0)\n",
    "\n",
    "        c1 = self.regressor.maxpool0(c0)\n",
    "        c1 = self.regressor.conv1(c1)\n",
    "        c1 = self.regressor.dropout1(c1)\n",
    "        c1 = self.regressor.bn1(c1)\n",
    "        c1 = self.regressor.elu1(c1)\n",
    "\n",
    "        c2 = self.regressor.maxpool1(c1)\n",
    "        c2 = self.regressor.conv2(c2)\n",
    "        c2 = self.regressor.bn2(c2)\n",
    "        c2 = self.regressor.elu2(c2)\n",
    "\n",
    "        c3 = self.regressor.maxpool2(c2)\n",
    "        c3 = self.regressor.conv3(c3)\n",
    "        c3 = self.regressor.dropout3(c3)\n",
    "        c3 = self.regressor.bn3(c3)\n",
    "        c3 = self.regressor.elu3(c3)\n",
    "\n",
    "        c4 = self.regressor.maxpool3(c3)\n",
    "        c4 = self.regressor.conv4(c4)\n",
    "        c4 = self.regressor.bn4(c4)\n",
    "        c4 = self.regressor.elu4(c4)\n",
    "\n",
    "        c5 = self.regressor.maxpool4(c4)\n",
    "        c5 = self.regressor.conv5(c5)\n",
    "        c5 = self.regressor.dropout5(c5)\n",
    "        c5 = self.regressor.bn5(c5)\n",
    "        c5 = self.regressor.elu5(c5)\n",
    "\n",
    "        c6 = self.regressor.maxpool5(c5)\n",
    "        c6 = self.regressor.conv6(c6)\n",
    "        c6 = self.regressor.elu6(c6)\n",
    "        h = self.regressor.flatten(c6)\n",
    "        h = tf.concat([h, hps], axis=-1)\n",
    "        h = self.regressor.dense(h)\n",
    "\n",
    "        slopes = self.re_slopes(z, training=training)\n",
    "        intercepts = self.re_intercept(z, training=training)\n",
    "        if len(slopes.shape) == 3:\n",
    "            slopes = tf.reduce_mean(slopes, axis=1)\n",
    "        if len(intercepts.shape) == 3:\n",
    "            intercepts = tf.reduce_mean(intercepts, axis=1)\n",
    "        y = self.regressor.out(h * (1 + slopes))\n",
    "\n",
    "        # Apply intercepts\n",
    "        y = y + intercepts\n",
    "\n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def compile(self,\n",
    "            loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "            loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "            loss_regressor_weight=1.0,\n",
    "            loss_gen_weight=0.1,\n",
    "            metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "            opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "            opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "            ):\n",
    "\n",
    "        super(MixedEffectsRegressor, self).compile(loss_regressor=loss_regressor,\n",
    "                                                    loss_adversary=loss_adversary,\n",
    "                                                    loss_regressor_weight=loss_regressor_weight,\n",
    "                                                    loss_gen_weight=loss_gen_weight,\n",
    "                                                    metric_regressor=metric_regressor,\n",
    "                                                    opt_regressor=opt_regressor,\n",
    "                                                    opt_adversary=opt_adversary)\n",
    "        self.loss_kld_tracker = tf.keras.metrics.Mean(name='kld')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_kld_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "\n",
    "    def _compute_update_loss(self, loss_reg, loss_gen, training=True):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "        if training:\n",
    "            kld = tf.reduce_sum(self.re_slopes.losses) + tf.reduce_sum(self.re_intercept.losses)\n",
    "            self.loss_kld_tracker.update_state(kld)\n",
    "        else:\n",
    "            kld = 0\n",
    "\n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) \\\n",
    "            + (self.loss_gen_weight * loss_gen) \\\n",
    "            + kld\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        sample_weights = None\n",
    "\n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "\n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            # 각 출력에 대한 손실을 계산하고 합산\n",
    "            total_loss_reg = 0\n",
    "            for i in range(y_pred.shape[1]): # y_pred의 출력 개수만큼 반복\n",
    "                total_loss_reg += self.loss_regressor(labels[:, i], y_pred[:, i], sample_weight=sample_weights)\n",
    "\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "            loss_total = self._compute_update_loss(total_loss_reg, loss_gen)\n",
    "\n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "\n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "\n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "\n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "\n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "\n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen, training=False)\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_keras(predictedGaze, groundtruthGaze, is_3d=False, deg=False):\n",
    "    '''\n",
    "    Calculate Mean Angular Error using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "    predictedGaze (tf.Tensor): Predicted gaze vectors.\n",
    "    groundtruthGaze (tf.Tensor): Ground truth gaze vectors.\n",
    "    is_3d (bool): Flag indicating whether the input vectors are 3D. Default is False.\n",
    "    deg (bool): Flag indicating whether the spherical coordinates are in degrees. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: Mean angular error.\n",
    "    '''\n",
    "    Gaze_1 = tf.cast(predictedGaze, dtype=tf.float32)\n",
    "    Gaze_2 = tf.cast(groundtruthGaze, dtype=tf.float32)\n",
    "\n",
    "    if not is_3d:\n",
    "        Gaze_1 = convert_to_xyz_tf(Gaze_1, deg)\n",
    "        Gaze_2 = convert_to_xyz_tf(Gaze_2, deg)\n",
    "\n",
    "    Gaze_1 = Gaze_1 / tf.norm(Gaze_1, axis=1, keepdims=True)\n",
    "    Gaze_2 = Gaze_2 / tf.norm(Gaze_2, axis=1, keepdims=True)\n",
    "\n",
    "    cos_val = tf.reduce_sum(Gaze_1 * Gaze_2, axis=1)\n",
    "    cos_val = tf.clip_by_value(cos_val, -1, 1)  # Ensure cos_val is within the valid range for arccos\n",
    "\n",
    "    angle_val = tf.acos(cos_val) * 180 / tf.constant(np.pi)\n",
    "\n",
    "    return tf.reduce_mean(angle_val)\n",
    "\n",
    "def convert_to_xyz_tf(spherical, deg=False):\n",
    "    if deg:\n",
    "        spherical = spherical * tf.constant(np.pi) / 180\n",
    "\n",
    "    # Create xyz tensor similarly as above but using TensorFlow operations.\n",
    "    xyz = tf.zeros((spherical.shape[0], 3), dtype=tf.float32)\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 0] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.sin(spherical[:, 1]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 1] for i in range(spherical.shape[0])],\n",
    "        -tf.sin(spherical[:, 0]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 2] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.cos(spherical[:, 1]))\n",
    "\n",
    "    xyz = xyz / tf.norm(xyz, axis=1, keepdims=True)\n",
    "\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "df = 3\n",
    "model = MixedEffectsRegressor(cluster_list = total_id_data, df = df)\n",
    "\n",
    "def t_loss(k):\n",
    "    def loss(y_true, y_pred, sample_weight=None):\n",
    "        error = y_true - y_pred\n",
    "        squared_error = tf.square(error)\n",
    "        scaled_error = tf.math.log(k + squared_error)\n",
    "        if sample_weight is not None:\n",
    "            scaled_error = tf.multiply(sample_weight, scaled_error)\n",
    "        return tf.reduce_mean(scaled_error)\n",
    "    return loss\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss_regressor=t_loss(df),\n",
    "              loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "              loss_regressor_weight=10.0,\n",
    "              loss_gen_weight=0.1,\n",
    "              metric_regressor=tf.keras.metrics.MeanAbsoluteError(),\n",
    "              opt_regressor=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              opt_adversary=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# 콜백 설정\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    x=train_data[0],  # 입력 데이터: 이미지와 클러스터 정보\n",
    "    y=train_data[1],  # 타겟 데이터: 시선 벡터\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=.2,  # 검증 데이터\n",
    "    # callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = ((test_img_normalized, loocv_hps_data_te, test_id_encoded), loocv_gzs_data_te)\n",
    "pred_gzs = model.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE 계산\n",
    "error = mae_keras(pred_gzs, test_data[1])\n",
    "print(\"Mean Angular Error:\", error.numpy(), \"degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 1\n",
    "# Mean Angular Error: 1.248835 degrees\n",
    "\n",
    "# Fold 2\n",
    "# Mean Angular Error: 1.2745246 degrees\n",
    "\n",
    "# Fold 3\n",
    "# Mean Angular Error: 1.6418864 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ONNX 변환을 위한 입력 시그니처 설정\n",
    "# input_signature = [\n",
    "#     tf.TensorSpec(shape=(None, 36, 60, 1), dtype=tf.float32, name='images'),\n",
    "#     tf.TensorSpec(shape=(None, 2), dtype=tf.float32, name='head'),\n",
    "#     tf.TensorSpec(shape=(None, 14), dtype=tf.float32, name='clusters')\n",
    "# ]\n",
    "# # ONNX 모델로 변환\n",
    "# onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature, opset=13)\n",
    "\n",
    "# # ONNX 모델 파일로 저장\n",
    "# onnx_file_path = \"./tARMED-CNN_best_240418.onnx\"\n",
    "# with open(onnx_file_path, \"wb\") as f:\n",
    "#     f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
