{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpii_path = \"/Users/dkmoon/Desktop/DKU/ML_LAB/Scholar/data/mpii\"\n",
    "\n",
    "# 전체 참가자 클래스에 대하여 Train/Valid 균등 분류\n",
    "within_id_data = np.load(os.path.join(mpii_path, \"within_ids.npy\"))\n",
    "# 'p' 제거\n",
    "within_id_data = np.char.replace(within_id_data, 'p', '')\n",
    "# 정수형으로 변환\n",
    "within_id_data = within_id_data.astype(int)\n",
    "within_hps_data = np.load(os.path.join(mpii_path, \"within_2d_hps.npy\"))\n",
    "within_img_data = np.load(os.path.join(mpii_path, \"within_images.npy\"))\n",
    "within_gzs_data = np.load(os.path.join(mpii_path, \"within_2d_gazes.npy\"))\n",
    "\n",
    "print(f\"within_id_data : {within_id_data.shape}\")\n",
    "print(f\"within_hps_data : {within_hps_data.shape}\")\n",
    "print(f\"within_img_data : {within_img_data.shape}\")\n",
    "print(f\"within_gzs_data : {within_gzs_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for splitting\n",
    "ids_data = within_id_data.reshape(-1)\n",
    "hps_data = within_hps_data.reshape(-1, 2)\n",
    "img_data = within_img_data.reshape(-1, 36, 60, 1) / 255.0\n",
    "gzs_data = within_gzs_data.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified k-fold splitter\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=530)\n",
    "\n",
    "# Split data\n",
    "folds = []\n",
    "for train_val_index, test_index in skf.split(np.zeros(ids_data.shape[0]), ids_data):\n",
    "    train_index, val_index = np.split(train_val_index, [int(len(train_val_index) * 0.875)])\n",
    "    \n",
    "    fold = {\n",
    "        'train': {\n",
    "            'ids': ids_data[train_index],\n",
    "            'hps': hps_data[train_index],\n",
    "            'imgs': img_data[train_index],\n",
    "            'gzs': gzs_data[train_index]\n",
    "        },\n",
    "        'val': {\n",
    "            'ids': ids_data[val_index],\n",
    "            'hps': hps_data[val_index],\n",
    "            'imgs': img_data[val_index],\n",
    "            'gzs': gzs_data[val_index]\n",
    "        },\n",
    "        'test': {\n",
    "            'ids': ids_data[test_index],\n",
    "            'hps': hps_data[test_index],\n",
    "            'imgs': img_data[test_index],\n",
    "            'gzs': gzs_data[test_index]\n",
    "        }\n",
    "    }\n",
    "    folds.append(fold)\n",
    "\n",
    "# Print fold information\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  Train set: {fold['train']['ids'].shape[0]} samples\")\n",
    "    print(f\"  Val set: {fold['val']['ids'].shape[0]} samples\")\n",
    "    print(f\"  Test set: {fold['test']['ids'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Effect Subnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedEffectSubnetwork(keras.Model):\n",
    "    def __init__(self, name='regressor', **kwargs):\n",
    "        super(FixedEffectSubnetwork, self).__init__(name=name, **kwargs)\n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.act0 = keras.layers.ELU(name='act0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        \n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(128, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.3, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.act1 = keras.layers.ELU(name='act1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        \n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.act2 = keras.layers.ELU(name='act2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        \n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.3, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.act3 = keras.layers.ELU(name='act3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        \n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.act4 = keras.layers.ELU(name='act4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        \n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(512, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.act5 = keras.layers.ELU(name='act5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        \n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.act6 = keras.layers.ELU(name='act6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu',\n",
    "                                        kernel_regularizer=keras.regularizers.L1L2(l1=0.01))\n",
    "        self.out = keras.layers.Dense(2, name='fe_output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        images, head_poses = inputs\n",
    "        \n",
    "        c0 = self.conv0(images)\n",
    "        c0 = self.bn0(c0)\n",
    "        c0 = self.act0(c0)\n",
    "        \n",
    "        c1 = self.maxpool0(c0)\n",
    "        c1 = self.conv1(c1)\n",
    "        c1 = self.dropout1(c1)\n",
    "        c1 = self.bn1(c1)\n",
    "        c1 = self.act1(c1)\n",
    "        \n",
    "        c2 = self.maxpool1(c1)\n",
    "        c2 = self.conv2(c2)\n",
    "        c2 = self.bn2(c2)\n",
    "        c2 = self.act2(c2)\n",
    "        \n",
    "        c3 = self.maxpool2(c2)\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = self.dropout3(c3)\n",
    "        c3 = self.bn3(c3)\n",
    "        c3 = self.act3(c3)\n",
    "        \n",
    "        c4 = self.maxpool3(c3)\n",
    "        c4 = self.conv4(c4)\n",
    "        c4 = self.bn4(c4)\n",
    "        c4 = self.act4(c4)\n",
    "        \n",
    "        c5 = self.maxpool4(c4)\n",
    "        c5 = self.conv5(c5)\n",
    "        c5 = self.dropout5(c5)\n",
    "        c5 = self.bn5(c5)\n",
    "        c5 = self.act5(c5)\n",
    "        \n",
    "        c6 = self.maxpool5(c5)\n",
    "        c6 = self.conv6(c6)\n",
    "        c6 = self.act6(c6)\n",
    "        \n",
    "        h = self.flatten(c6)\n",
    "        h = keras.layers.Concatenate()([h, head_poses])\n",
    "        h = self.dense(h)\n",
    "        y = self.out(h)\n",
    "        \n",
    "        return c0, c1, c2, c3, c4, c5, c6, h, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effect Subnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior_fn(df, scale):\n",
    "    def prior_fn():\n",
    "        return tfd.StudentT(df=df, loc=0.0, scale=scale)\n",
    "    return prior_fn\n",
    "\n",
    "def make_posterior_fn(df, loc_init_scale, scale_init_min, scale_init_range):\n",
    "    def posterior_fn(units):\n",
    "        loc_initializer = tf.random_normal_initializer(mean=0.0, stddev=loc_init_scale)\n",
    "        scale_initializer = tf.random_uniform_initializer(minval=scale_init_min, maxval=scale_init_min + scale_init_range)\n",
    "\n",
    "        loc = tf.Variable(initial_value=loc_initializer(shape=(units,)), name=\"loc\", dtype=tf.float32)\n",
    "        scale = tf.Variable(initial_value=scale_initializer(shape=(units,)), name=\"scale\", dtype=tf.float32)\n",
    "\n",
    "        return tfd.StudentT(df=df, loc=loc, scale=tf.nn.softplus(scale))\n",
    "    return posterior_fn\n",
    "\n",
    "def kl_divergence_student_t(posterior, prior, num_samples=1000, seed=530):\n",
    "    \"\"\"Calculate the KL divergence between two Student's T distributions using Monte Carlo approximation.\"\"\"\n",
    "    samples = posterior.sample(num_samples, seed=seed)  # Sample from the posterior\n",
    "    log_posterior_prob = posterior.log_prob(samples)  # Log probability under the posterior\n",
    "    log_prior_prob = prior.log_prob(samples)  # Log probability under the prior\n",
    "    \n",
    "    # Monte Carlo approximation of the KL divergence\n",
    "    kl_div = tf.reduce_mean(log_posterior_prob - log_prior_prob)\n",
    "    return kl_div\n",
    "\n",
    "class RandomEffectSubnetwork(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 df,\n",
    "                 post_loc_init_scale, \n",
    "                 post_scale_init_min, \n",
    "                 post_scale_init_range,\n",
    "                 prior_scale,\n",
    "                 kl_weight=0.001, \n",
    "                 l1_weight=None, \n",
    "                 name=None):\n",
    "\n",
    "        super(RandomEffectSubnetwork, self).__init__(name=name)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.units = units\n",
    "\n",
    "        self.posterior = make_posterior_fn(df, post_loc_init_scale, post_scale_init_min, post_scale_init_range)(units)\n",
    "        self.prior = make_prior_fn(df, prior_scale)()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)  # 데이터 타입을 float32로 변경\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)  # inputs의 차원을 확장\n",
    "        if training:\n",
    "            # Shape of `samples`: [batch_size, units]\n",
    "            samples = self.posterior.sample(sample_shape=(tf.shape(inputs)[0],))\n",
    "            # Ensure 'samples' can be broadcasted with 'inputs'\n",
    "            # Assuming `inputs` shape is [batch_size, input_dim], we need to align 'samples' along that dimension\n",
    "            samples = tf.reshape(samples, [tf.shape(inputs)[0], 1, self.units])\n",
    "            outputs = inputs * samples  # Broadcast multiplication\n",
    "\n",
    "            kl_div = kl_divergence_student_t(self.posterior, self.prior, num_samples=500, seed=530)\n",
    "            self.add_loss(self.kl_weight * tf.reduce_sum(kl_div))\n",
    "        else:\n",
    "            # Use the mean of the posterior distribution as a deterministic output\n",
    "            mean_samples = self.posterior.mean()\n",
    "            mean_samples = tf.reshape(mean_samples, [1, 1, self.units])\n",
    "            outputs = inputs * mean_samples  # Broadcast multiplication\n",
    "\n",
    "        if self.l1_weight:\n",
    "            self.add_loss(self.l1_weight * tf.reduce_sum(tf.abs(self.posterior.mean())))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialRegressor(keras.Model):\n",
    "    def __init__(self, n_clusters, name='adversary', **kwargs):   \n",
    "        super(AdversarialRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.act0 = keras.layers.ELU(name='act0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.3, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.act1 = keras.layers.ELU(name='act1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.act2 = keras.layers.ELU(name='act2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(128, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.3, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.act3 = keras.layers.ELU(name='act3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.act4 = keras.layers.ELU(name='act4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(256, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.3, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.act5 = keras.layers.ELU(name='act5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.act6 = keras.layers.ELU(name='act6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        \n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu')\n",
    "        self.out = keras.layers.Dense(n_clusters, name='adv_output', activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Fixed Effect Subnetwork의 Feature map\n",
    "        c0, c1, c2, c3, c4, c5, c6, h = inputs\n",
    "        \n",
    "        x = self.conv0(c0)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act0(x)\n",
    "        x = self.maxpool0(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c2])\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c3])\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c4])\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c5])\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act5(x)\n",
    "        x = self.maxpool5(x)\n",
    "\n",
    "        x = keras.layers.Concatenate()([x, c6])\n",
    "        x = self.conv6(x)\n",
    "        x = self.act6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = keras.layers.Concatenate()([x, h])\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x) # z_pred\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Effect Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEffectNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 cluster_list=[],\n",
    "                 slope_post_init_scale=0.1,\n",
    "                 intercept_post_init_scale=0.1,\n",
    "                 slope_scale=0.25,\n",
    "                 intercept_scale=0.25,\n",
    "                 kl_weight=1e-3,\n",
    "                 df=3,\n",
    "                 name='me_network', **kwargs):\n",
    "        super(MixedEffectNetwork, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        # 고정 효과 하위 네트워크 함수 정의\n",
    "        self.fixed_effect_subnetwork = FixedEffectSubnetwork(name='fe_network')\n",
    "        \n",
    "        # 변량 효과 하위 네트워크의 기울기 함수 정의\n",
    "        self.re_slopes = RandomEffectSubnetwork(units=256,\n",
    "                                                df=df,\n",
    "                                                post_loc_init_scale=slope_post_init_scale,\n",
    "                                                post_scale_init_min=0.01, \n",
    "                                                post_scale_init_range=0.02,\n",
    "                                                prior_scale=slope_scale,\n",
    "                                                kl_weight=kl_weight,\n",
    "                                                name='re_slopes')\n",
    "        # 변량 효과 하위 네트워크의 절편 함수 정의\n",
    "        self.re_intercept = RandomEffectSubnetwork(units=2,\n",
    "                                                   df=df,\n",
    "                                                   post_loc_init_scale=intercept_post_init_scale,\n",
    "                                                   post_scale_init_min=0.01, \n",
    "                                                   post_scale_init_range=0.02,\n",
    "                                                   prior_scale=intercept_scale,\n",
    "                                                   kl_weight=kl_weight,\n",
    "                                                   name='re_intercept')\n",
    "        \n",
    "        # 적대적 분류 신경망\n",
    "        self.cluster_list = tf.constant(cluster_list, dtype=tf.int64)\n",
    "        n_clusters= len(self.cluster_list)\n",
    "        self.adversary = AdversarialRegressor(n_clusters, name='z_predictor')\n",
    "        \n",
    "        # 혼합 효과 출력\n",
    "        self.me_out = keras.layers.Dense(2, name='me_output')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        images, head_poses, cluster_ids = inputs\n",
    "        # Fixed Effect Subnetwork\n",
    "        c0, c1, c2, c3, c4, c5, c6, h, y_fixed = self.fixed_effect_subnetwork((images, head_poses))\n",
    "        \n",
    "        # Random Effect Subnetwork\n",
    "        re_slope = self.re_slopes(cluster_ids, training=training)\n",
    "        re_intercept = self.re_intercept(cluster_ids, training=training)\n",
    "        \n",
    "        if len(re_slope.shape) == 3:\n",
    "            re_slope = tf.reduce_mean(re_slope, axis=1)\n",
    "        if len(re_intercept.shape) == 3:\n",
    "            re_intercept = tf.reduce_mean(re_intercept, axis=1)\n",
    "        \n",
    "        # Mixed Effect Network\n",
    "        y_mixed = self.me_out(h * (1 + re_slope))\n",
    "        y_mixed = y_mixed + re_intercept\n",
    "        \n",
    "        return c0, c1, c2, c3, c4, c5, c6, h, y_fixed, re_slope, re_intercept, y_mixed\n",
    "    \n",
    "    def compile(self,\n",
    "                loss_me_regressor=keras.losses.MeanAbsoluteError(),\n",
    "                loss_fe_regressor=keras.losses.MeanAbsoluteError(),\n",
    "                loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "                fe_regressor_weight=1.0,\n",
    "                adv_gen_weight=0.1,\n",
    "                \n",
    "                metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "                opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "                opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)):\n",
    "        \n",
    "        super(MixedEffectNetwork, self).compile()\n",
    "        \n",
    "        self.loss_me_regressor = loss_me_regressor\n",
    "        self.loss_fe_regressor = loss_fe_regressor\n",
    "        self.loss_adversary = loss_adversary\n",
    "        self.fe_regressor_weight = fe_regressor_weight\n",
    "        self.adv_gen_weight = adv_gen_weight\n",
    "        \n",
    "        self.metric_regressor = metric_regressor\n",
    "        self.opt_regressor = opt_regressor\n",
    "        self.opt_adversary = opt_adversary\n",
    "        \n",
    "        self.loss_me_regressor_tracker = tf.keras.metrics.Mean(name='me_regressor_loss')\n",
    "        self.loss_fe_regressor_tracker = tf.keras.metrics.Mean(name='fe_regressor_loss')\n",
    "        self.loss_adversary_tracker = tf.keras.metrics.Mean(name='adversary_loss')\n",
    "        self.loss_kld_tracker = tf.keras.metrics.Mean(name='kld')\n",
    "        self.loss_total_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_me_regressor_tracker,\n",
    "                self.loss_fe_regressor_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_kld_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "\n",
    "    def _compute_update_loss(self, loss_me_reg, loss_fe_reg, loss_adv, training=True):\n",
    "        self.loss_me_regressor_tracker.update_state(loss_me_reg)\n",
    "        self.loss_fe_regressor_tracker.update_state(loss_fe_reg)\n",
    "        \n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "        if training:\n",
    "            kld = tf.reduce_sum(self.re_slopes.losses) + tf.reduce_sum(self.re_intercept.losses)\n",
    "            self.loss_kld_tracker.update_state(kld)\n",
    "        else:\n",
    "            # KLD can't be computed at inference time because posteriors are simplified to \n",
    "            # point estimates\n",
    "            kld = 0\n",
    "            \n",
    "        loss_total = loss_me_reg + (self.fe_regressor_weight * loss_fe_reg) + (self.adv_gen_weight * loss_adv) + kld\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    # 모델 학습\n",
    "    def train_step(self, data):\n",
    "        (images, head_poses, clusters), labels = data\n",
    "        sample_weights = None\n",
    "\n",
    "        # 원-핫 인코딩된 클러스터\n",
    "        clusters_one_hot = to_categorical(clusters, num_classes=len(self.cluster_list))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            c0, c1, c2, c3, c4, c5, c6, h, y_fixed, re_slope, re_intercept, y_mixed = self((images, head_poses, clusters), training=True)\n",
    "            clusters_pred = self.adversary([c0, c1, c2, c3, c4, c5, c6, h])\n",
    "            loss_adv = self.loss_adversary(clusters_one_hot, clusters_pred, sample_weight=sample_weights)\n",
    "            \n",
    "            loss_fe_reg = self.loss_fe_regressor(labels, y_fixed)\n",
    "            loss_me_reg = self.loss_me_regressor(labels, y_mixed)\n",
    "            loss_total = self._compute_update_loss(loss_me_reg, loss_fe_reg, loss_adv, training=True)\n",
    "            \n",
    "        grads = tape.gradient(loss_total, self.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_mixed)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        \n",
    "        # 원-핫 인코딩된 클러스터\n",
    "        clusters_one_hot = to_categorical(clusters, num_classes=len(self.cluster_list))\n",
    "\n",
    "        # Z Predict\n",
    "        c0, c1, c2, c3, c4, c5, c6, h, y_fixed, re_slope, re_intercept, y_mixed = self((images, headpose, clusters), training=False)\n",
    "        \n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters_one_hot, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "        \n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary([c0, c1, c2, c3, c4, c5, c6, h])\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters_one_hot, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        _, _, _, _, _, _, _, _, y_fixed_new, re_slope_new, re_intercept_new, y_mixed_new = self((images, headpose, new_clusters), training=False)\n",
    "        \n",
    "        loss_me_reg = self.loss_me_regressor(labels, y_mixed_new)\n",
    "        loss_fe_reg = self.loss_fe_regressor(labels, y_fixed_new)\n",
    "        loss_adv = self.loss_adversary(new_clusters, adversary_output)\n",
    "        \n",
    "        _ = self._compute_update_loss(loss_me_reg, loss_fe_reg, loss_adv, training=False)\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_mixed_new)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_keras(predictedGaze, groundtruthGaze, is_3d=False, deg=False):\n",
    "    '''\n",
    "    Calculate Mean Angular Error using TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "    predictedGaze (tf.Tensor): Predicted gaze vectors.\n",
    "    groundtruthGaze (tf.Tensor): Ground truth gaze vectors.\n",
    "    is_3d (bool): Flag indicating whether the input vectors are 3D. Default is False.\n",
    "    deg (bool): Flag indicating whether the spherical coordinates are in degrees. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    tf.Tensor: Mean angular error.\n",
    "    '''\n",
    "    Gaze_1 = tf.cast(predictedGaze, dtype=tf.float32)\n",
    "    Gaze_2 = tf.cast(groundtruthGaze, dtype=tf.float32)\n",
    "    \n",
    "    if not is_3d:\n",
    "        Gaze_1 = convert_to_xyz_tf(Gaze_1, deg)\n",
    "        Gaze_2 = convert_to_xyz_tf(Gaze_2, deg)\n",
    "\n",
    "    Gaze_1 = Gaze_1 / tf.norm(Gaze_1, axis=1, keepdims=True)\n",
    "    Gaze_2 = Gaze_2 / tf.norm(Gaze_2, axis=1, keepdims=True)\n",
    "\n",
    "    cos_val = tf.reduce_sum(Gaze_1 * Gaze_2, axis=1)\n",
    "    cos_val = tf.clip_by_value(cos_val, -1, 1)  # Ensure cos_val is within the valid range for arccos\n",
    "\n",
    "    angle_val = tf.acos(cos_val) * 180 / tf.constant(np.pi)\n",
    "\n",
    "    return tf.reduce_mean(angle_val)\n",
    "\n",
    "def convert_to_xyz_tf(spherical, deg=False):\n",
    "    if deg:\n",
    "        spherical = spherical * tf.constant(np.pi) / 180\n",
    "\n",
    "    # Create xyz tensor similarly as above but using TensorFlow operations.\n",
    "    xyz = tf.zeros((spherical.shape[0], 3), dtype=tf.float32)\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 0] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.sin(spherical[:, 1]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 1] for i in range(spherical.shape[0])],\n",
    "        -tf.sin(spherical[:, 0]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 2] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.cos(spherical[:, 1]))\n",
    "\n",
    "    xyz = xyz / tf.norm(xyz, axis=1, keepdims=True)\n",
    "\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-분포를 따르는 손실 함수 정의\n",
    "def t_loss(k):\n",
    "    def loss(y_true, y_pred, sample_weight=None):\n",
    "        error = y_true - y_pred\n",
    "        squared_error = tf.square(error)\n",
    "        scaled_error = tf.math.log(k + squared_error)\n",
    "        if sample_weight is not None:\n",
    "            scaled_error = tf.multiply(sample_weight, scaled_error)\n",
    "        return tf.reduce_mean(scaled_error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 3\n",
    "unique_ids = np.unique(ids_data)\n",
    "\n",
    "# Model training and evaluation using K-fold cross-validation\n",
    "mae_list = []\n",
    "histories = []\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"Training fold {i+1}...\")\n",
    "\n",
    "    # Create the model\n",
    "    model = MixedEffectNetwork(cluster_list=unique_ids, df=df)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss_me_regressor=t_loss(df),\n",
    "        loss_fe_regressor=t_loss(df),\n",
    "        loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "        fe_regressor_weight=1.0,\n",
    "        adv_gen_weight=0.1,\n",
    "        metric_regressor=tf.keras.metrics.MeanAbsoluteError(),\n",
    "        opt_regressor=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        opt_adversary=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    )\n",
    "\n",
    "    # Prepare training and validation data\n",
    "    train_data = (fold['train']['imgs'], fold['train']['hps'], fold['train']['ids']), fold['train']['gzs']\n",
    "    val_data = (fold['val']['imgs'], fold['val']['hps'], fold['val']['ids']), fold['val']['gzs']\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=train_data[0],\n",
    "        y=train_data[1],\n",
    "        epochs=350,\n",
    "        batch_size=32,\n",
    "        validation_data=val_data,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    histories.append(history)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_data = (fold['test']['imgs'], fold['test']['hps'], fold['test']['ids']), fold['test']['gzs']\n",
    "    pred_gzs = model.predict(test_data[0])\n",
    "    \n",
    "    mae_error = mae_keras(pred_gzs, test_data[1])\n",
    "    mae_list.append(mae_error)\n",
    "    print(f\"Test results for fold {i+1}: {mae_error}\")\n",
    "\n",
    "print(f\"\\nMean Error : {sum(mae_list)/len(mae_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
