{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, BatchNormalization, PReLU, MaxPool2D, Dropout, Dense, Flatten\n",
    "from keras.regularizers import L1L2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "# GPU 메모리 증가 허용 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpii_path = \"../data\"\n",
    "\n",
    "# 전체 참가자 클래스에 대하여 Train/Valid 균등 분류\n",
    "within_id_data = np.load(os.path.join(mpii_path, \"within_ids.npy\"))\n",
    "within_hps_data = np.load(os.path.join(mpii_path, \"within_2d_hps.npy\"))\n",
    "within_img_data = np.load(os.path.join(mpii_path, \"within_images.npy\"))\n",
    "within_gzs_data = np.load(os.path.join(mpii_path, \"within_2d_gazes.npy\"))\n",
    "\n",
    "print(f\"within_id_data : {within_id_data.shape}\")\n",
    "print(f\"within_hps_data : {within_hps_data.shape}\")\n",
    "print(f\"within_img_data : {within_img_data.shape}\")\n",
    "print(f\"within_gzs_data : {within_gzs_data.shape}\\n\")\n",
    "\n",
    "# ID 데이터를 정수 인덱스로 변환\n",
    "unique_ids, id_indices = np.unique(within_id_data, return_inverse=True)\n",
    "n_clusters = len(unique_ids)  # 클러스터(참가자) 수\n",
    "\n",
    "# 원-핫 인코딩으로 클러스터 정보 변환\n",
    "id_one_hot = to_categorical(id_indices).reshape(within_id_data.shape[0], within_id_data.shape[1], -1)\n",
    "id_one_hot = id_one_hot.reshape(-1, n_clusters)  # 샘플 수, 클러스터 수\n",
    "\n",
    "# 클러스터 데이터 형태 변환\n",
    "ids_data = within_id_data.reshape(-1,1)\n",
    "l_enc = LabelEncoder()\n",
    "ids_enc_data = l_enc.fit_transform(ids_data)\n",
    "\n",
    "# 이미지 데이터 정규화 및 형태 변환\n",
    "normalized_img_data = within_img_data / 255.0\n",
    "normalized_img_data = normalized_img_data.reshape(-1, 36, 60, 1)  # 샘플 수, 높이, 너비, 채널\n",
    "\n",
    "# 헤드포즈 데이터 형태 변환\n",
    "hps_data = within_hps_data.reshape(-1, 2)  # 샘플 수, 헤드포즈 차원\n",
    "\n",
    "# 시선 정보 형태 변환\n",
    "gzs_data = within_gzs_data.reshape(-1, 2)  # 샘플 수, 시선 정보 차원\n",
    "\n",
    "# 데이터셋 분할 (예시: 80% 훈련, 10% 검증, 10% 테스트)\n",
    "total_samples = normalized_img_data.shape[0]\n",
    "train_samples = int(total_samples * 0.9)\n",
    "# val_samples = int(total_samples * 0.1)\n",
    "\n",
    "train_id = id_one_hot[:train_samples]\n",
    "train_img = normalized_img_data[:train_samples]\n",
    "train_hps = hps_data[:train_samples]\n",
    "train_gzs = gzs_data[:train_samples]\n",
    "\n",
    "# val_id = id_one_hot[train_samples:train_samples + val_samples]\n",
    "# val_img = normalized_img_data[train_samples:train_samples + val_samples]\n",
    "# val_hps = hps_data[train_samples:train_samples + val_samples]\n",
    "# val_gzs = gzs_data[train_samples:train_samples + val_samples]\n",
    "\n",
    "# test_id = id_one_hot[train_samples + val_samples:]\n",
    "# test_img = normalized_img_data[train_samples + val_samples:]\n",
    "# test_hps = hps_data[train_samples + val_samples:]\n",
    "# test_gzs = gzs_data[train_samples + val_samples:]\n",
    "\n",
    "test_id = id_one_hot[train_samples:]\n",
    "test_img = normalized_img_data[train_samples:]\n",
    "test_hps = hps_data[train_samples:]\n",
    "test_gzs = gzs_data[train_samples:]\n",
    "\n",
    "# 학습 데이터, 검증 데이터, 테스트 데이터 준비\n",
    "train_data = ((train_img, train_hps, train_id), train_gzs)\n",
    "# val_data = ((val_img, val_hps, val_id), val_gzs)\n",
    "test_data = ((test_img, test_hps, test_id), test_gzs)\n",
    "\n",
    "# 데이터 shape 출력\n",
    "print(\"Train data shape:\")\n",
    "print(\"  Clusters:\", train_id.shape)\n",
    "print(\"  Images:\", train_img.shape)\n",
    "print(\"  Head:\", train_hps.shape)\n",
    "print(\"  Gazes:\", train_gzs.shape)\n",
    "\n",
    "# print(\"Validation data shape:\")\n",
    "# print(\"  Clusters:\", val_id.shape)\n",
    "# print(\"  Images:\", val_img.shape)\n",
    "# print(\"  Head:\", val_hps.shape)\n",
    "# print(\"  Gazes:\", val_gzs.shape)\n",
    "\n",
    "print(\"Test data shape:\")\n",
    "print(\"  Clusters:\", test_id.shape)\n",
    "print(\"  Images:\", test_img.shape)\n",
    "print(\"  Head:\", test_hps.shape)\n",
    "print(\"  Gazes:\", test_gzs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRegressor(keras.Model):\n",
    "    def __init__(self, name='regressor', **kwargs):\n",
    "        super(ImageRegressor, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.act0 = keras.layers.ELU(name='act0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(128, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.act1 = keras.layers.ELU(name='act1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.act2 = keras.layers.ELU(name='act2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.act3 = keras.layers.ELU(name='act3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.act4 = keras.layers.ELU(name='act4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(512, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.act5 = keras.layers.ELU(name='act5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.act6 = keras.layers.ELU(name='act6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu',\n",
    "                                        kernel_regularizer=keras.regularizers.L1L2(l1=0.01))\n",
    "        self.out = keras.layers.Dense(2, name='output')\n",
    "        \n",
    "    def call(self, inputs, return_layer_activations=False):\n",
    "        images, head_poses = inputs\n",
    "        \n",
    "        c0 = self.conv0(images)\n",
    "        c0 = self.bn0(c0)\n",
    "        c0 = self.act0(c0)\n",
    "        \n",
    "        c1 = self.maxpool0(c0)\n",
    "        c1 = self.conv1(c1)\n",
    "        c1 = self.dropout1(c1)\n",
    "        c1 = self.bn1(c1)\n",
    "        c1 = self.act1(c1)\n",
    "        \n",
    "        c2 = self.maxpool1(c1)\n",
    "        c2 = self.conv2(c2)\n",
    "        c2 = self.bn2(c2)\n",
    "        c2 = self.act2(c2)\n",
    "        \n",
    "        c3 = self.maxpool2(c2)\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = self.dropout3(c3)\n",
    "        c3 = self.bn3(c3)\n",
    "        c3 = self.act3(c3)\n",
    "        \n",
    "        c4 = self.maxpool3(c3)\n",
    "        c4 = self.conv4(c4)\n",
    "        c4 = self.bn4(c4)\n",
    "        c4 = self.act4(c4)\n",
    "        \n",
    "        c5 = self.maxpool4(c4)\n",
    "        c5 = self.conv5(c5)\n",
    "        c5 = self.dropout5(c5)\n",
    "        c5 = self.bn5(c5)\n",
    "        c5 = self.act5(c5)\n",
    "        \n",
    "        c6 = self.maxpool5(c5)\n",
    "        c6 = self.conv6(c6)\n",
    "        c6 = self.act6(c6)\n",
    "        h = self.flatten(c6)\n",
    "        h = keras.layers.Concatenate()([h, head_poses])\n",
    "        h = self.dense(h)\n",
    "        y = self.out(h)\n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialRegressor(keras.Model):\n",
    "    def __init__(self, n_clusters, name='adversary', **kwargs):   \n",
    "        super(AdversarialRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        # 36 x 60\n",
    "        self.conv0 = keras.layers.Conv2D(64, 3, padding='same', name='conv0')\n",
    "        self.bn0 = keras.layers.BatchNormalization(name='bn0')\n",
    "        self.act0 = keras.layers.ELU(name='act0')\n",
    "        self.maxpool0 = keras.layers.MaxPool2D(name='maxpool0')\n",
    "        # 18 x 30        \n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same', name='conv1')\n",
    "        self.dropout1 = keras.layers.Dropout(0.5, name='dropout1')\n",
    "        self.bn1 = keras.layers.BatchNormalization(name='bn1')\n",
    "        self.act1 = keras.layers.ELU(name='act1')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(name='maxpool1')\n",
    "        # 9 x 15\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same', name='conv2')\n",
    "        self.bn2 = keras.layers.BatchNormalization(name='bn2')\n",
    "        self.act2 = keras.layers.ELU(name='act2')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(name='maxpool2')\n",
    "        # 5 x 8\n",
    "        self.conv3 = keras.layers.Conv2D(128, 3, padding='same', name='conv3')\n",
    "        self.dropout3 = keras.layers.Dropout(0.5, name='dropout3')\n",
    "        self.bn3 = keras.layers.BatchNormalization(name='bn3')\n",
    "        self.act3 = keras.layers.ELU(name='act3')\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(name='maxpool3')\n",
    "        # 3 x 4\n",
    "        self.conv4 = keras.layers.Conv2D(256, 3, padding='same', name='conv4')\n",
    "        self.bn4 = keras.layers.BatchNormalization(name='bn4')\n",
    "        self.act4 = keras.layers.ELU(name='act4')\n",
    "        self.maxpool4 = keras.layers.MaxPool2D(name='maxpool4')\n",
    "        # 2 x 2\n",
    "        self.conv5 = keras.layers.Conv2D(256, 3, padding='same', name='conv5')\n",
    "        self.dropout5 = keras.layers.Dropout(0.5, name='dropout5')\n",
    "        self.bn5 = keras.layers.BatchNormalization(name='bn5')\n",
    "        self.act5 = keras.layers.ELU(name='act5')\n",
    "        self.maxpool5 = keras.layers.MaxPool2D(padding='same', name='maxpool5')\n",
    "        # # 1 x 1\n",
    "        self.conv6 = keras.layers.Conv2D(512, 3, padding='same', name='conv6')\n",
    "        self.act6 = keras.layers.ELU(name='act6')\n",
    "        self.flatten = keras.layers.Flatten(name='flatten')\n",
    "        \n",
    "        self.dense = keras.layers.Dense(256, name='dense', activation='elu')\n",
    "        self.out = keras.layers.Dense(n_clusters, name='output', activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Fixed Effect Subnetwork의 Feature map\n",
    "        c0, c1, c2, c3, c4, c5, c6, h = inputs\n",
    "        \n",
    "        x = self.conv0(c0)\n",
    "        x = self.bn0(x)\n",
    "        x = self.act0(x)\n",
    "        x = self.maxpool0(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c2])\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c3])\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c4])\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate()([x, c5])\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act5(x)\n",
    "        \n",
    "        x = self.maxpool5(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.act6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = keras.layers.Concatenate()([x, h])\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Effects Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior_fn(df, scale):\n",
    "    def prior_fn():\n",
    "        return tfd.StudentT(df=df, loc=0.0, scale=scale)\n",
    "    return prior_fn\n",
    "\n",
    "def make_posterior_fn(df, loc_init_scale, scale_init_min, scale_init_range):\n",
    "    def posterior_fn(units):\n",
    "        loc_initializer = tf.random_normal_initializer(mean=0.0, stddev=loc_init_scale)\n",
    "        scale_initializer = tf.random_uniform_initializer(minval=scale_init_min, maxval=scale_init_min + scale_init_range)\n",
    "\n",
    "        loc = tf.Variable(initial_value=loc_initializer(shape=(units,)), name=\"loc\", dtype=tf.float32)\n",
    "        scale = tf.Variable(initial_value=scale_initializer(shape=(units,)), name=\"scale\", dtype=tf.float32)\n",
    "\n",
    "        return tfd.StudentT(df=df, loc=loc, scale=tf.nn.softplus(scale))\n",
    "    return posterior_fn\n",
    "\n",
    "def kl_divergence_student_t(posterior, prior, num_samples=1000, seed=None):\n",
    "    \"\"\"Calculate the KL divergence between two Student's T distributions using Monte Carlo approximation.\"\"\"\n",
    "    samples = posterior.sample(num_samples, seed=seed)  # Sample from the posterior\n",
    "    log_posterior_prob = posterior.log_prob(samples)  # Log probability under the posterior\n",
    "    log_prior_prob = prior.log_prob(samples)  # Log probability under the prior\n",
    "    \n",
    "    # Monte Carlo approximation of the KL divergence\n",
    "    kl_div = tf.reduce_mean(log_posterior_prob - log_prior_prob)\n",
    "    return kl_div\n",
    "\n",
    "class RandomEffects(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 df,\n",
    "                 post_loc_init_scale, \n",
    "                 post_scale_init_min, \n",
    "                 post_scale_init_range,\n",
    "                 prior_scale,\n",
    "                 kl_weight=0.001, \n",
    "                 l1_weight=None, \n",
    "                 name=None):\n",
    "\n",
    "        super(RandomEffects, self).__init__(name=name)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.units = units\n",
    "\n",
    "        self.posterior = make_posterior_fn(df, post_loc_init_scale, post_scale_init_min, post_scale_init_range)(units)\n",
    "        self.prior = make_prior_fn(df, prior_scale)()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)  # 데이터 타입을 float32로 변경\n",
    "        inputs = tf.expand_dims(inputs, axis=-1)  # inputs의 차원을 확장\n",
    "        if training:\n",
    "            # Shape of `samples`: [batch_size, units]\n",
    "            samples = self.posterior.sample(sample_shape=(tf.shape(inputs)[0],))\n",
    "            # Ensure 'samples' can be broadcasted with 'inputs'\n",
    "            # Assuming `inputs` shape is [batch_size, input_dim], we need to align 'samples' along that dimension\n",
    "            samples = tf.reshape(samples, [tf.shape(inputs)[0], 1, self.units])\n",
    "            outputs = inputs * samples  # Broadcast multiplication\n",
    "\n",
    "            kl_div = kl_divergence_student_t(self.posterior, self.prior, num_samples=1000, seed=42)\n",
    "            self.add_loss(self.kl_weight * tf.reduce_sum(kl_div))\n",
    "        else:\n",
    "            # Use the mean of the posterior distribution as a deterministic output\n",
    "            mean_samples = self.posterior.mean()\n",
    "            mean_samples = tf.reshape(mean_samples, [1, 1, self.units])\n",
    "            outputs = inputs * mean_samples  # Broadcast multiplication\n",
    "\n",
    "        if self.l1_weight:\n",
    "            self.add_loss(self.l1_weight * tf.reduce_sum(tf.abs(self.posterior.mean())))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adversarial Image Regressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdversarialImageRegressor(keras.Model):\n",
    "\n",
    "    def __init__(self, n_clusters, name='da_regressor', **kwargs):  \n",
    "        super(DomainAdversarialImageRegressor, self).__init__(name=name, **kwargs)\n",
    "        self.regressor = ImageRegressor()\n",
    "        self.adversary = AdversarialRegressor(n_clusters)\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, hps, z = inputs\n",
    "        y_pred = self.regressor((x,hps))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def compile(self,\n",
    "                loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "                loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "                loss_regressor_weight=1.0,\n",
    "                loss_gen_weight=0.1,\n",
    "                metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "                opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "                opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "                ):\n",
    "      \n",
    "        super().compile()\n",
    "        \n",
    "        self.loss_regressor = loss_regressor\n",
    "        self.loss_adversary = loss_adversary\n",
    "        self.loss_regressor_weight = loss_regressor_weight\n",
    "        self.loss_gen_weight = loss_gen_weight\n",
    "        self.metric_regressor = metric_regressor\n",
    "        self.opt_regressor = opt_regressor\n",
    "        self.opt_adversary = opt_adversary\n",
    "        \n",
    "        self.loss_regressor_tracker = keras.metrics.Mean(name='reg_loss')\n",
    "        self.loss_gen_tracker = keras.metrics.Mean(name='gen_loss')\n",
    "        self.loss_adversary_tracker = keras.metrics.Mean(name='adv_loss')\n",
    "        self.loss_total_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "        \n",
    "    def _compute_update_loss(self, loss_reg, loss_gen):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "        \n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) + (self.loss_gen_weight * loss_gen)\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "        \n",
    "        return loss_total\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            (images, headpose, clusters), labels, sample_weights = data\n",
    "        else:\n",
    "            (images, headpose, clusters), labels = data\n",
    "            sample_weights = None\n",
    "            \n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "        \n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self.regressor((images, headpose), return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            loss_reg = self.loss_regressor(labels, y_pred, sample_weight=sample_weights)\n",
    "                        \n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "            \n",
    "            loss_total = self._compute_update_loss(loss_reg, loss_gen)\n",
    "            \n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        \n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "        \n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "        \n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "        \n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "        \n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen)\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Effects Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEffectsRegressor(DomainAdversarialImageRegressor):\n",
    "    def __init__(self, \n",
    "                 cluster_list=[],\n",
    "                 slope_post_init_scale=0.1,\n",
    "                 intercept_post_init_scale=0.1,\n",
    "                 slope_scale=0.25,\n",
    "                 intercept_scale=0.25,\n",
    "                 kl_weight=1e-3,\n",
    "                 df=10,\n",
    "                 name='me_regressor', **kwargs):\n",
    "\n",
    "        self.cluster_list = tf.constant([int(cid[1:]) for cid in cluster_list], dtype=tf.int64)\n",
    "        n_clusters = len(self.cluster_list)\n",
    "        super(MixedEffectsRegressor, self).__init__(n_clusters, name=name, **kwargs)\n",
    "                \n",
    "        self.re_slopes = RandomEffects(units=256,\n",
    "                                       df=df,\n",
    "                                       post_loc_init_scale=slope_post_init_scale,\n",
    "                                       post_scale_init_min=0.01, \n",
    "                                       post_scale_init_range=0.02,\n",
    "                                       prior_scale=slope_scale,\n",
    "                                       kl_weight=kl_weight,\n",
    "                                       name='re_slopes')\n",
    "        \n",
    "        self.re_intercept = RandomEffects(units=1,\n",
    "                                          df=df,\n",
    "                                          post_loc_init_scale=intercept_post_init_scale,\n",
    "                                          post_scale_init_min=0.01, \n",
    "                                          post_scale_init_range=0.02,\n",
    "                                          prior_scale=intercept_scale,\n",
    "                                          kl_weight=kl_weight,\n",
    "                                          name='re_intercept')\n",
    "        \n",
    "    def call(self, inputs, training, return_layer_activations=False):\n",
    "        x, hps, z = inputs\n",
    "        \n",
    "        if x.shape[-1] != 1:\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        c0 = self.regressor.conv0(x)\n",
    "        c0 = self.regressor.bn0(c0)\n",
    "        c0 = self.regressor.act0(c0)\n",
    "        \n",
    "        c1 = self.regressor.maxpool0(c0)\n",
    "        c1 = self.regressor.conv1(c1)\n",
    "        c1 = self.regressor.dropout1(c1)\n",
    "        c1 = self.regressor.bn1(c1)\n",
    "        c1 = self.regressor.act1(c1)\n",
    "        \n",
    "        c2 = self.regressor.maxpool1(c1)\n",
    "        c2 = self.regressor.conv2(c2)\n",
    "        c2 = self.regressor.bn2(c2)\n",
    "        c2 = self.regressor.act2(c2)\n",
    "        \n",
    "        c3 = self.regressor.maxpool2(c2)\n",
    "        c3 = self.regressor.conv3(c3)\n",
    "        c3 = self.regressor.dropout3(c3)\n",
    "        c3 = self.regressor.bn3(c3)\n",
    "        c3 = self.regressor.act3(c3)\n",
    "        \n",
    "        c4 = self.regressor.maxpool3(c3)\n",
    "        c4 = self.regressor.conv4(c4)\n",
    "        c4 = self.regressor.bn4(c4)\n",
    "        c4 = self.regressor.act4(c4)\n",
    "        \n",
    "        c5 = self.regressor.maxpool4(c4)\n",
    "        c5 = self.regressor.conv5(c5)\n",
    "        c5 = self.regressor.dropout5(c5)\n",
    "        c5 = self.regressor.bn5(c5)\n",
    "        c5 = self.regressor.act5(c5)\n",
    "        \n",
    "        c6 = self.regressor.maxpool5(c5)\n",
    "        c6 = self.regressor.conv6(c6)\n",
    "        c6 = self.regressor.act6(c6)\n",
    "        h = self.regressor.flatten(c6)\n",
    "        h = tf.concat([h, hps], axis=-1)\n",
    "        h = self.regressor.dense(h)\n",
    "        \n",
    "        slopes = self.re_slopes(z, training=training)\n",
    "        intercepts = self.re_intercept(z, training=training)\n",
    "        if len(slopes.shape) == 3:\n",
    "            slopes = tf.reduce_mean(slopes, axis=1)\n",
    "        if len(intercepts.shape) == 3:\n",
    "            intercepts = tf.reduce_mean(intercepts, axis=1)\n",
    "        y = self.regressor.out(h * (1 + slopes))\n",
    "\n",
    "        # Apply intercepts\n",
    "        y = y + intercepts\n",
    "        \n",
    "        if return_layer_activations:\n",
    "            return c0, c1, c2, c3, c4, c5, c6, h, y\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def compile(self,\n",
    "            loss_regressor=keras.losses.MeanAbsoluteError(),\n",
    "            loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "            loss_regressor_weight=1.0,\n",
    "            loss_gen_weight=0.1,\n",
    "            metric_regressor=keras.metrics.MeanAbsoluteError(),\n",
    "            opt_regressor=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "            opt_adversary=keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "            ):\n",
    "        \n",
    "        super(MixedEffectsRegressor, self).compile(loss_regressor=loss_regressor,\n",
    "                                                    loss_adversary=loss_adversary,\n",
    "                                                    loss_regressor_weight=loss_regressor_weight,\n",
    "                                                    loss_gen_weight=loss_gen_weight,\n",
    "                                                    metric_regressor=metric_regressor,\n",
    "                                                    opt_regressor=opt_regressor,\n",
    "                                                    opt_adversary=opt_adversary)\n",
    "        self.loss_kld_tracker = tf.keras.metrics.Mean(name='kld')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_regressor_tracker,\n",
    "                self.loss_gen_tracker,\n",
    "                self.loss_adversary_tracker,\n",
    "                self.loss_kld_tracker,\n",
    "                self.loss_total_tracker,\n",
    "                self.metric_regressor]\n",
    "        \n",
    "    def _compute_update_loss(self, loss_reg, loss_gen, training=True):\n",
    "        '''Compute total loss and update loss running means'''\n",
    "        self.loss_regressor_tracker.update_state(loss_reg)\n",
    "        self.loss_gen_tracker.update_state(loss_gen)\n",
    "        if training:\n",
    "            kld = tf.reduce_sum(self.re_slopes.losses) + tf.reduce_sum(self.re_intercept.losses)\n",
    "            self.loss_kld_tracker.update_state(kld)\n",
    "        else:\n",
    "            kld = 0\n",
    "        loss_total = (self.loss_regressor_weight * loss_reg) \\\n",
    "            + (self.loss_gen_weight * loss_gen) \\\n",
    "            + kld\n",
    "        self.loss_total_tracker.update_state(loss_total)\n",
    "        \n",
    "        return loss_total\n",
    "        \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        sample_weights = None\n",
    "\n",
    "        # train adversary\n",
    "        with tf.GradientTape() as gt:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_adv = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "        grads_adv = gt.gradient(loss_adv, self.adversary.trainable_weights)\n",
    "        self.opt_adversary.apply_gradients(zip(grads_adv, self.adversary.trainable_weights))\n",
    "        self.loss_adversary_tracker.update_state(loss_adv)\n",
    "\n",
    "        # train main regressor\n",
    "        with tf.GradientTape() as gt2:\n",
    "            reg_outs = self((images, headpose, clusters), training=True, return_layer_activations=True)\n",
    "            y_pred = reg_outs[-1]\n",
    "            # 각 출력에 대한 손실을 계산하고 합산\n",
    "            total_loss_reg = 0\n",
    "            for i in range(y_pred.shape[1]):  # y_pred의 출력 개수만큼 반복\n",
    "                if sample_weights is not None:\n",
    "                    total_loss_reg += self.loss_regressor(labels[:, i], y_pred[:, i], sample_weight=sample_weights[:, i])\n",
    "                else:\n",
    "                    total_loss_reg += self.loss_regressor(labels[:, i], y_pred[:, i])\n",
    "\n",
    "            layer_activations = reg_outs[:-1]\n",
    "            clusters_pred = self.adversary(layer_activations)\n",
    "            loss_gen = self.loss_adversary(clusters, clusters_pred, sample_weight=sample_weights)\n",
    "\n",
    "            loss_total = self._compute_update_loss(total_loss_reg, loss_gen)\n",
    "\n",
    "        grads = gt2.gradient(loss_total, self.regressor.trainable_weights)\n",
    "        self.opt_regressor.apply_gradients(zip(grads, self.regressor.trainable_weights))\n",
    "\n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (images, headpose, clusters), labels = data\n",
    "        \n",
    "        # Z Predict\n",
    "        reg_outs = self((images, headpose, clusters), training=False, return_layer_activations=True)\n",
    "        layer_activations = reg_outs[:-1]\n",
    "        \n",
    "        # 클러스터 인덱스 추출\n",
    "        cluster_indices = tf.argmax(clusters, axis=1)\n",
    "\n",
    "        # Check if cluster IDs are known\n",
    "        is_known_cluster = tf.vectorized_map(lambda x: tf.reduce_any(tf.equal(x, self.cluster_list)), cluster_indices)\n",
    "        \n",
    "        # Calculate adversary output once and reuse\n",
    "        adversary_output = self.adversary(layer_activations)\n",
    "\n",
    "        # Use known or predicted clusters\n",
    "        new_clusters = tf.where(tf.expand_dims(is_known_cluster, axis=1), clusters, adversary_output)\n",
    "\n",
    "        # Main Regression with potentially new clusters\n",
    "        reg_outs = self((images, headpose, new_clusters), training=False, return_layer_activations=True)\n",
    "        y_pred = reg_outs[-1]\n",
    "        \n",
    "        loss_reg = self.loss_regressor(labels, y_pred)\n",
    "        loss_gen = self.loss_adversary(new_clusters, adversary_output)\n",
    "        \n",
    "        _ = self._compute_update_loss(loss_reg, loss_gen)\n",
    "        \n",
    "        self.metric_regressor.update_state(labels, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_keras(predictedGaze, groundtruthGaze, is_3d=False, deg=False):\n",
    "    '''\n",
    "    Calculate Mean Angular Error using TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "    predictedGaze (tf.Tensor): Predicted gaze vectors.\n",
    "    groundtruthGaze (tf.Tensor): Ground truth gaze vectors.\n",
    "    is_3d (bool): Flag indicating whether the input vectors are 3D. Default is False.\n",
    "    deg (bool): Flag indicating whether the spherical coordinates are in degrees. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    tf.Tensor: Mean angular error.\n",
    "    '''\n",
    "    Gaze_1 = tf.cast(predictedGaze, dtype=tf.float32)\n",
    "    Gaze_2 = tf.cast(groundtruthGaze, dtype=tf.float32)\n",
    "    \n",
    "    if not is_3d:\n",
    "        Gaze_1 = convert_to_xyz_tf(Gaze_1, deg)\n",
    "        Gaze_2 = convert_to_xyz_tf(Gaze_2, deg)\n",
    "\n",
    "    Gaze_1 = Gaze_1 / tf.norm(Gaze_1, axis=1, keepdims=True)\n",
    "    Gaze_2 = Gaze_2 / tf.norm(Gaze_2, axis=1, keepdims=True)\n",
    "\n",
    "    cos_val = tf.reduce_sum(Gaze_1 * Gaze_2, axis=1)\n",
    "    cos_val = tf.clip_by_value(cos_val, -1, 1)  # Ensure cos_val is within the valid range for arccos\n",
    "\n",
    "    angle_val = tf.acos(cos_val) * 180 / tf.constant(np.pi)\n",
    "\n",
    "    return tf.reduce_mean(angle_val)\n",
    "\n",
    "def convert_to_xyz_tf(spherical, deg=False):\n",
    "    if deg:\n",
    "        spherical = spherical * tf.constant(np.pi) / 180\n",
    "\n",
    "    # Create xyz tensor similarly as above but using TensorFlow operations.\n",
    "    xyz = tf.zeros((spherical.shape[0], 3), dtype=tf.float32)\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 0] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.sin(spherical[:, 1]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 1] for i in range(spherical.shape[0])],\n",
    "        -tf.sin(spherical[:, 0]))\n",
    "    xyz = tf.tensor_scatter_nd_update(\n",
    "        xyz, [[i, 2] for i in range(spherical.shape[0])],\n",
    "        -tf.cos(spherical[:, 0]) * tf.cos(spherical[:, 1]))\n",
    "\n",
    "    xyz = xyz / tf.norm(xyz, axis=1, keepdims=True)\n",
    "\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "df = 3\n",
    "model = MixedEffectsRegressor(cluster_list = unique_ids, df = df)\n",
    "\n",
    "def t_loss(k):\n",
    "    def loss(y_true, y_pred, sample_weight=None):\n",
    "        error = y_true - y_pred\n",
    "        squared_error = tf.square(error)\n",
    "        scaled_error = tf.math.log(k + squared_error)\n",
    "        if sample_weight is not None:\n",
    "            scaled_error = tf.multiply(sample_weight, scaled_error)\n",
    "        return tf.reduce_mean(scaled_error)\n",
    "    return loss\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss_regressor=t_loss(df),\n",
    "              loss_adversary=keras.losses.CategoricalCrossentropy(),\n",
    "              loss_regressor_weight=7.5,\n",
    "              loss_gen_weight=0.5,\n",
    "              metric_regressor=tf.keras.metrics.MeanAbsoluteError(),\n",
    "              opt_regressor=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              opt_adversary=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# 콜백 설정\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    x=train_data[0],  # 입력 데이터: 이미지와 클러스터 정보\n",
    "    y=train_data[1],  # 타겟 데이터: 시선 벡터\n",
    "    epochs=350,\n",
    "    batch_size=32,\n",
    "    # validation_data=val_data,  # 검증 데이터\n",
    "    # callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gzs = model.predict((test_img, test_hps, test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predict Sample : {pred_gzs[-5:]}\")\n",
    "print(f\"True Sample : {test_gzs[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE 계산\n",
    "error = mae_keras(pred_gzs, test_gzs)\n",
    "print(\"Mean Angular Error:\", error.numpy(), \"degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
